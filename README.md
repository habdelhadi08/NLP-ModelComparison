ğŸ“Š Coleridge Initiative NLP Classification Project
Welcome to the Coleridge Initiative NLP Project!
This repository presents deep learning models (BiLSTM, GRU, CNN) developed to classify cleaned text labels extracted from research publications and datasets.
The goal is to build a robust text classification pipeline capable of handling short, domain-specific text entries using modern NLP techniques.

ğŸ—‚ï¸ Project Structure
ğŸ“ Coleridge Initiative Project/
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ cleaned_train.csv
â”‚   â””â”€â”€ train.csv
â”œâ”€â”€ images
â”œâ”€â”€ coleridge_initiative_model.ipynb  â† Model training & evaluation
â”œâ”€â”€ dataset_analysis.ipynb            â† EDA & insights
â”œâ”€â”€ requirements.txt
â”‚â”€â”€.venv/                             â† Virtual environment
â””â”€â”€ README.md

ğŸ“Œ Objectives
- ğŸ§¼ Clean and preprocess publication and dataset title text

- ğŸ”  Tokenize and pad sequences using Keras

- ğŸ§  Build and compare 3 deep learning architectures:

    - ğŸ§¬ BiLSTM
    - ğŸ” GRU
    - ğŸ“¶ CNN

- ğŸ“ˆ Visualize training progress and confusion matrices

- âš–ï¸ Address class imbalance using SMOTE

| Category          | Tools / Libraries                      |
| ----------------- | -------------------------------------- |
| **Language**      | Python 3.11                            |
| **DL Framework**  | TensorFlow / Keras                     |
| **ML Tools**      | Scikit-learn, SMOTE (Imbalanced-learn) |
| **Visualization** | Plotly                                 |
| **Notebook**      | Jupyter Notebook                       |

ğŸ”„ Data Pipeline
1- ğŸ”— Merge pub_title + dataset_title

2- ğŸ”¡ Tokenize & Pad using Keras Tokenizer

3- ğŸ¯ Encode cleaned_label with LabelEncoder + One-Hot

4- ğŸ” Stratified Train/Test Split

5- ğŸ“ˆ Balance Classes with SMOTE

6- ğŸ§  Train BiLSTM, GRU, and CNN

7- ğŸ§ª Evaluate via classification report + confusion matrix

ğŸ§  Model Architectures
ğŸ§¬ BiLSTM
- Embedding â†’ Bidirectional LSTM â†’ Global Max Pool â†’ Dense

- Handles long-range dependencies in text

ğŸ” GRU
- Embedding â†’ GRU Layer â†’ Global Max Pool â†’ Dense

- Faster and simpler than LSTM with competitive results

ğŸ“¶ CNN
- Embedding â†’ 1D Convolution â†’ Max Pooling â†’ Flatten â†’ Dense

- Excels at capturing local features in text (like n-grams)

ğŸ“Š Visualizations
ğŸ¯ Training Accuracy
![Alt text](images/BiLSTM_Model.png)
![Alt text](images/GRU_Model.png)
![Alt text](images/CNN_Model.png)



ğŸš€ Getting Started
1. Clone the repo

```bash
git clone https://github.com/habdelhadi08/NLP-ModelComparison.git
cd Coleridge Initiative Project
```
2. Set up environment

```bash
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows
pip install -r requirements.txt
```

3. Run notebook
Launch the Jupyter Notebook:

```bash
jupyter notebook dataset_analysis.ipynb
jupyter notebook coleridge_initiative_model.ipynb
```

âœ… Results Summary
| Model  | Accuracy (Val) | Strengths                                                     |
| ------ | -------------- | ------------------------------------------------------------- |
| BiLSTM | \~73%          | Best performance on sequence data; captures long dependencies |
| GRU    | \~74%          | Fewer parameters, faster training, good generalization        |
| CNN    | \~72%          | Captures local word patterns effectively with fast training   |

ğŸ›  Limitations
- The dataset remains challenging due to its imbalance and short text length.
- Models tend to confuse semantically similar labels.
- Validation accuracy capped around 68â€“74% despite hyperparameter tuning.

ğŸ Conclusion
This project highlights the effectiveness of neural network architecturesâ€”BiLSTM, GRU, and CNNâ€”for classifying short text sequences.
Despite the inherent challenges of working with limited context and class imbalance, the models achieved validation accuracies ranging from 72% to 74%.

GRU emerged as a strong performer, offering competitive accuracy with efficient training time.
Future improvements may include exploring transformer-based models or integrating pre-trained embeddings (like GloVe or BERT).

This work demonstrates how deep learning techniques can support accurate classification of concise text data, enabling smarter indexing and retrieval in textual applications.

ğŸ“¬ Contact
Data Scientist: Heba Abdelhadi
ğŸ“§ habdelhadi08@gmail.com
ğŸ“ Shelby Township, MI
ğŸ“… Capstone Project for Qwasar Data Science & Machine Learning Bootcamp